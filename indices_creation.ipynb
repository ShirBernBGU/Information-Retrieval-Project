{"cells":[{"cell_type":"markdown","id":"cc8c5918","metadata":{},"source":["# setups"]},{"cell_type":"code","execution_count":1,"id":"caac7a24","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","cluster-8096  GCE       3                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"code","execution_count":2,"id":"576b6555","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":3,"id":"9cebc982","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import builtins\n","import math\n","from nltk.stem import *\n","\n","\n","stemmer = PorterStemmer()\n","\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"id":"33baec24","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  7 14:39 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":5,"id":"e1ffe587","metadata":{},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":6,"id":"0f992aba","metadata":{},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-8096-m.c.ir-hw3-413816.internal:44457\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f6cab0b04c0>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","spark"]},{"cell_type":"markdown","id":"dbadb915","metadata":{},"source":["# Building an Inverted Index for title with stemming and ngram"]},{"cell_type":"markdown","id":"eac207e5","metadata":{},"source":["### check access to the bucket, change the \"bucket_name\" depending on the bucket"]},{"cell_type":"code","execution_count":null,"id":"617d5c11","metadata":{},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = '209706803_body' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and not b.name.startswith(\"postings_gcp\"):\n","        paths.append(full_path+b.name)"]},{"cell_type":"code","execution_count":12,"id":"8f95c26e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs_limit = parquetFile.select(\"title\", \"id\").rdd "]},{"cell_type":"code","execution_count":13,"id":"cc158b5a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Row(title='Foster Air Force Base', id=4045403)\n"]}],"source":["# Count number of wiki pages - Careful, might kill kernel\n","parquetFile.count()\n"]},{"cell_type":"markdown","id":"8d2b326e","metadata":{},"source":["### Nirs imports - make sure Inverted Index class exists and is imported"]},{"cell_type":"code","execution_count":17,"id":"a062722b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":18,"id":"0689b4d5","metadata":{},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":19,"id":"53093092","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"0283a143","metadata":{},"source":["#### depends on what kind of tokenizing desired, change the arguments of the function tokenizer"]},{"cell_type":"code","execution_count":20,"id":"a2669eab","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","\n","def tokenizer(text, stem=True, ngram=True):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    \n","    if stem and not ngram:\n","        tokens = [stemmer.stem(token) for token in tokens if token not in all_stopwords]\n","    \n","    elif ngram and not stem:\n","        tokens = [token[0] + \" \" + token[1] for token in list(nltk.bigrams(tokens))]\n","    \n","    elif stem and ngram:\n","        tokens = [stemmer.stem(token) for token in tokens if token not in all_stopwords]\n","        tokens = [token[0] + \" \" + token[1] for token in list(nltk.bigrams(tokens))]\n","        \n","    else:\n","        tokens = [token for token in tokens if (token not in all_stopwords)]\n","        \n","    return tokens"]},{"cell_type":"markdown","id":"975d4a77","metadata":{},"source":["# in the code below, depending in the index being created, change the variables \"BASE_DIR\" and \"BUCKET_NAME\""]},{"cell_type":"code","execution_count":21,"id":"e1225cfb","metadata":{},"outputs":[],"source":["\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","\n","\n","BASE_DIR = \"POSTING_LISTS_TITLE\"\n","BUCKET_NAME = \"209706803\"\n","\n","\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in\n","  `all_stopwords` and return entries that will go into our posting lists.\n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","  tokens = tokenizer(text)\n","\n","  # YOUR CODE HERE\n","  counting_dict = Counter(tokens)\n","\n","  lst_of_tups = [(key, (id, val)) for key, val in counting_dict.items()]\n","\n","  return lst_of_tups\n","\n","\n","def reduce_word_counts(unsorted_pl):\n","  ''' Returns a sorted posting list by wiki_id.\n","  Parameters:\n","  -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","  Returns:\n","  --------\n","    list of tuples\n","      A sorted posting list.\n","  '''\n","  # YOUR CODE HERE\n","  return sorted(unsorted_pl, key=lambda x: x[0])\n","\n","\n","def calculate_df(postings):\n","  ''' Takes a posting list RDD and calculate the df for each token.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.\n","  '''\n","  # YOUR CODE HERE \n","    \n","  token_df = postings.mapValues(lambda x: len(x))\n","\n","  return token_df\n","\n","\n","def partition_postings_and_write(postings, BASE_DIR):\n","  ''' A function that partitions the posting lists into buckets, writes out\n","  all posting lists in a bucket to disk, and returns the posting locations for\n","  each bucket. Partitioning should be done through the use of `token2bucket`\n","  above. Writing to disk should use the function  `write_a_posting_list`, a\n","  static method implemented in inverted_index_colab.py under the InvertedIndex\n","  class.\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","  Returns:\n","  --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","  '''\n","\n","  # YOUR CODE HERE\n","\n","  # Step 1: Determine the bucket ID for each token - (bucket_id, (token, pl))\n","  bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n","\n","  # Step 2: Group posting lists by bucket ID\n","  grouped_buckets = bucketed_postings.groupByKey()\n","\n","  # Step 3: Write each posting list to disk and collect posting locations - (location(bucket_id), (token, pl))\n","  posting_locations = grouped_buckets.map(lambda x: InvertedIndex().write_a_posting_list((x[0], x[1]), BASE_DIR, BUCKET_NAME))\n","\n","  return posting_locations\n"]},{"cell_type":"markdown","id":"c27cefb0","metadata":{},"source":["# Before running the code below, check the tokenizer used in the variable \"id_title_length_dict\" and adjust the arguments accordingly"]},{"cell_type":"code","execution_count":23,"id":"cde7f6f2","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# SHIR AND NITZAN - TITLE INVERTED INDEX WITH STEMMING AND NGRAM CALCULATIONS\n","\n","# time the index creation time\n","t_start = time()\n","# word counts map\n","word_counts = doc_title_pairs_limit.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","# filtering postings and calculate df\n","\n","################3\n","postings_filtered = postings.filter(lambda x: len(x[1])>=1)\n","################3\n","\n","id_title_length_list = doc_title_pairs_limit.map(lambda row: (row.id, len(tokenizer(row.title)))).collect()\n","id_title_length_dict = {item[0]: item[1] for item in id_title_length_list}\n","\n","\n","w2df = calculate_df(postings_filtered)\n","w2df_dict = w2df.collectAsMap()\n","# partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered, BASE_DIR).collect()\n","index_const_time = time() - t_start"]},{"cell_type":"code","execution_count":24,"id":"331c3c8b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["187.73400020599365\n"]}],"source":["# test index construction time\n","print(index_const_time)"]},{"cell_type":"markdown","id":"989718d0","metadata":{},"source":["## change the \"POSTING_DIRECTORY\" to the name of the directory the posting lists will be located"]},{"cell_type":"code","execution_count":22,"id":"41554430","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","\n","POSTING_DIRECTORY = \"POSTING_LISTS_TITLE\"\n","\n","super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=POSTING_DIRECTORY):\n","  if not blob.name.endswith(\"pickle\"):\n","    continue\n","  with blob.open(\"rb\") as f:\n","    posting_locs = pickle.load(f)\n","    for k, v in posting_locs.items():\n","      super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":23,"id":"a991c04d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n","Operation completed over 1 objects/1.5 KiB.                                      \n"]}],"source":["# Create inverted index instance  \n","\n","\n","########## we may want to address the fact that we are not updating the term_total attribute #########\n","inverted = InvertedIndex()\n","# Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = super_posting_locs\n","# Add the token - df dictionary to the inverted index\n","inverted.df = w2df_dict\n","######## Add the DL dict\n","inverted.DL = id_title_length_dict\n","# write the global stats out\n","inverted.write_index('.', 'index')\n","# upload to gs\n","index_src = \"index.pkl\"\n","index_dst = f'gs://{bucket_name}/{POSTING_DIRECTORY}/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":24,"id":"7e953b95","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  1.48 KiB  2024-03-05T22:44:17Z  gs://209706803/POSTING_LISTS_TITLE/index.pkl\r\n","TOTAL: 1 objects, 1513 bytes (1.48 KiB)\r\n"]}],"source":["!gsutil ls -lh $index_dst"]},{"cell_type":"markdown","id":"e80036e0","metadata":{},"source":["## tests to see if the attributes are set correctly"]},{"cell_type":"code","execution_count":39,"id":"c0d78673","metadata":{},"outputs":[{"data":{"text/plain":["1090"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["inverted.DL"]},{"cell_type":"code","execution_count":null,"id":"9181b4da","metadata":{},"outputs":[],"source":["inverted.posting_locs"]},{"cell_type":"code","execution_count":null,"id":"2da72d72","metadata":{},"outputs":[],"source":["inverted.df"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}